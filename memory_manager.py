#!/usr/bin/env python3
"""
í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
- ëª¨ë“  AI ëª¨ë¸ì˜ ë¡œë“œ/ì–¸ë¡œë“œ ê´€ë¦¬
- GPU ë©”ëª¨ë¦¬ ì •ë¦¬
- í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
"""

import subprocess
import os
import gc
import psutil
import signal

class MemoryManager:
    def __init__(self):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.loaded_models = {
            'whisper': None,
            'llava_active': False,
            'tinyllama_active': False,
            'tts_processes': []
        }
    
    def unload_whisper_model(self):
        """Whisper ëª¨ë¸ ì–¸ë¡œë“œ"""
        try:
            # function_question.pyì˜ ì „ì—­ ë³€ìˆ˜ ì ‘ê·¼
            import function.function_question as fq
            if hasattr(fq, 'whisper_model') and fq.whisper_model is not None:
                print("ğŸ§¹ Whisper ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
                del fq.whisper_model
                fq.whisper_model = None
                self.loaded_models['whisper'] = None
                print("âœ… Whisper ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
                return True
        except Exception as e:
            print(f"âš ï¸ Whisper ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    
    def stop_all_ollama_models(self):
        """ëª¨ë“  Ollama ëª¨ë¸ ì¤‘ì§€"""
        models_to_stop = ['llava', 'tinyllama']
        stopped_models = []
        
        for model in models_to_stop:
            try:
                print(f"ğŸ›‘ {model} ëª¨ë¸ ì¤‘ì§€ ì¤‘...")
                result = subprocess.run(
                    ['ollama', 'stop', model], 
                    capture_output=True, 
                    text=True, 
                    timeout=10
                )
                if result.returncode == 0:
                    print(f"âœ… {model} ëª¨ë¸ ì¤‘ì§€ ì™„ë£Œ")
                    stopped_models.append(model)
                    self.loaded_models[f'{model}_active'] = False
                else:
                    print(f"âš ï¸ {model} ëª¨ë¸ ì¤‘ì§€ ì‹¤íŒ¨: {result.stderr}")
            except subprocess.TimeoutExpired:
                print(f"âš ï¸ {model} ëª¨ë¸ ì¤‘ì§€ íƒ€ì„ì•„ì›ƒ")
            except Exception as e:
                print(f"âš ï¸ {model} ëª¨ë¸ ì¤‘ì§€ ì˜¤ë¥˜: {e}")
        
        return stopped_models
    
    def kill_all_tts_processes(self):
        """ëª¨ë“  TTS ê´€ë ¨ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ"""
        tts_patterns = [
            'tts_cli.py',
            'GPTSoVits', 
            'GPT-SoVITS',
            'tts_server'
        ]
        
        killed_count = 0
        for pattern in tts_patterns:
            try:
                print(f"ğŸ”« TTS í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì¤‘: {pattern}")
                result = subprocess.run(
                    ['pkill', '-f', pattern], 
                    capture_output=True, 
                    text=True
                )
                if result.returncode == 0:
                    killed_count += 1
                    print(f"âœ… {pattern} í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ {pattern} í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì˜¤ë¥˜: {e}")
        
        return killed_count > 0
    
    def kill_tablet_processes(self):
        """íƒœë¸”ë¦¿ ê´€ë ¨ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ"""
        try:
            # ì‹¤í–‰ ì¤‘ì¸ íƒœë¸”ë¦¿ í”„ë¡œì„¸ìŠ¤ ì°¾ê¸°
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    if 'tablet' in cmdline.lower() or 'coordinate' in cmdline.lower():
                        print(f"ğŸ”« íƒœë¸”ë¦¿ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ: PID {proc.info['pid']}")
                        proc.terminate()
                        proc.wait(timeout=3)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            print("âœ… íƒœë¸”ë¦¿ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âš ï¸ íƒœë¸”ë¦¿ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì˜¤ë¥˜: {e}")
            return False
    
    def clear_gpu_memory(self):
        """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            import torch
            if torch.cuda.is_available():
                print("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì¤‘...")
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                print("âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                return True
        except ImportError:
            print("â„¹ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ê±´ë„ˆëœ€")
        except Exception as e:
            print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì˜¤ë¥˜: {e}")
        return False
    
    def clear_system_memory(self):
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            print("ğŸ§¹ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì¤‘...")
            gc.collect()
            print("âœ… ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âš ï¸ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def emergency_memory_cleanup(self):
        """ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ - ëª¨ë“  ëª¨ë¸ê³¼ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ"""
        print("ğŸš¨ ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...")
        
        cleanup_results = {
            'whisper': self.unload_whisper_model(),
            'ollama': len(self.stop_all_ollama_models()) > 0,
            'tts': self.kill_all_tts_processes(),
            'tablet': self.kill_tablet_processes(),
            'gpu': self.clear_gpu_memory(),
            'system': self.clear_system_memory()
        }
        
        success_count = sum(1 for success in cleanup_results.values() if success)
        total_count = len(cleanup_results)
        
        print(f"ğŸ¯ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {success_count}/{total_count} ì‘ì—… ì„±ê³µ")
        print("=" * 50)
        
        return cleanup_results
    
    def get_memory_status(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            gpu_info = "N/A"
            
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
                    gpu_cached = torch.cuda.memory_reserved() / 1024**3   # GB
                    gpu_info = f"GPU: {gpu_memory:.1f}GB ì‚¬ìš©, {gpu_cached:.1f}GB ìºì‹œ"
            except:
                pass
            
            print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìƒíƒœ:")
            print(f"   RAM: {memory.percent}% ì‚¬ìš© ({memory.used/1024**3:.1f}GB/{memory.total/1024**3:.1f}GB)")
            print(f"   {gpu_info}")
            
            return {
                'ram_percent': memory.percent,
                'ram_used_gb': memory.used/1024**3,
                'ram_total_gb': memory.total/1024**3,
                'gpu_info': gpu_info
            }
        except Exception as e:
            print(f"âš ï¸ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
            return None

# ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
memory_manager = MemoryManager()

# í¸ì˜ í•¨ìˆ˜ë“¤
def emergency_exit_to_main():
    """ì·¨ì†Œ ë²„íŠ¼ ì‹œ í˜¸ì¶œ - ëª¨ë“  ë¦¬ì†ŒìŠ¤ í•´ì œí•˜ê³  ë©”ì¸ìœ¼ë¡œ"""
    print("ğŸ”„ ë©”ì¸ ë©”ë‰´ë¡œ ë¹„ìƒ ë³µê·€ ì¤‘...")
    cleanup_results = memory_manager.emergency_memory_cleanup()
    return cleanup_results

def get_memory_status():
    """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
    return memory_manager.get_memory_status()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    get_memory_status()
    print("\nê¸´ê¸‰ ì •ë¦¬ í…ŒìŠ¤íŠ¸")
    emergency_exit_to_main()